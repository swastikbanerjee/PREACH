{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d123c3c-0581-4b4d-a745-a51b3f17efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install transformers\n",
    "# %pip install torch\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34c8eb9-76fc-4195-bddf-37b5ce7056a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: 3 star\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "text = \"the dustbin is good but the amazing sticker is bad\"\n",
    "inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "_, predicted_sentiment = torch.max(probabilities, dim=-1)\n",
    "sentiment_labels = [\"1 star\", \"2 star\", \"3 star\", \"4 star\", \"5 star\"]\n",
    "print(f\"Predicted Sentiment: {sentiment_labels[predicted_sentiment]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec583187-cc9a-48c5-88d7-170efaf7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained('Intel/neural-chat-7b-v3-1')\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained('Intel/neural-chat-7b-v3-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdb8cd-bc08-4d0f-91cc-b6b8bac14a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response(system_input, user_input):\n",
    "#     # Format the input using the provided template\n",
    "#     prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "#     # Tokenize and encode the prompt\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "#     # Generate a response\n",
    "#     outputs = model.generate(inputs, max_length=3500, num_return_sequences=1, pad_token_id = tokenizer.eos_token_id)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     # Extract only the assistant's response\n",
    "#     return response.split(\"### Assistant:\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9279763f-06f3-4a25-9b7a-5f96e24d7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = 'AIzaSyBileOLS4Ys9Nk1X27OqBccsEmgxOWmV54'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860ca0b4-e19f-4760-927a-f0203d4cddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667974eb-90aa-4532-893c-427510d44582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de4aa61-086e-4715-ba1a-5e6a53981462",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9f98c1-1074-4ce8-9713-a8af23dffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b1b5145-c75a-40b7-b0ac-e2579a95c789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In conclusion, Gemini is the culmination of an extensive collaborative endeavor, drawing upon the expertise of teams throughout Google, including those at Google Research. It has been meticulously designed with multimodality at its core, enabling it to seamlessly comprehend, navigate, and fuse diverse forms of information, encompassing text, code, audio, images, and videos.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \\\n",
    "'''\n",
    "I am going to give you a summary report. You have to understand the sentiment of the summary and then elaborate on it.\n",
    "Text: Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.\n",
    "(Start with: In conclusion...)\n",
    "'''\n",
    "response = model.generate_content(text)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7f556-ef76-49e2-b0df-c3a2cdc53964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (IntelÂ® oneAPI 2024)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-pytorch-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
